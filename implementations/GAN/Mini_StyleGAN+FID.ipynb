{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Style GAN\n",
        "\n",
        "Auteurs : Lisa Giordani, Mouïn Ben Ammar, Yoldoz Tabei, Ilias Harkati (Groupe 6)\n",
        "\n",
        "Cours : Projet IA (IA321)\n",
        "\n",
        "Projet : Génération d'images (P13)\n",
        "\n",
        "Date : Mars 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:21:43.703191Z",
          "iopub.status.busy": "2022-03-13T12:21:43.702219Z",
          "iopub.status.idle": "2022-03-13T12:21:46.324162Z",
          "shell.execute_reply": "2022-03-13T12:21:46.323451Z",
          "shell.execute_reply.started": "2022-03-13T12:21:43.703068Z"
        },
        "id": "V2ahXlGdU0NI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from scipy.stats import truncnorm\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# the name of the files in which we'll save our models\n",
        "Pkl_gen = \"gen.pkl\"  \n",
        "Pkl_critic = \"critic.pkl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVtvjHltTzfi"
      },
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:21:46.325946Z",
          "iopub.status.busy": "2022-03-13T12:21:46.325704Z",
          "iopub.status.idle": "2022-03-13T12:21:46.337064Z",
          "shell.execute_reply": "2022-03-13T12:21:46.336248Z",
          "shell.execute_reply.started": "2022-03-13T12:21:46.325911Z"
        },
        "id": "6hfO5l28Tn00",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def show_tensor_images(image_tensor,epoch, num_images=25, size=(1, 28, 28)):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in a uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "def get_truncated_noise(n_samples, z_dim, truncation):# truncate the sampled noise based on how much variability is desired\n",
        "    truncated_noise = truncnorm.rvs(-truncation, truncation, size=(n_samples, z_dim))\n",
        "    return torch.Tensor(truncated_noise)\n",
        "\n",
        "def plot_to_tensorboard(\n",
        "    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n",
        "):\n",
        "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # take out (up to) 8 examples to plot\n",
        "        img_grid_real = torchvision.utils.make_grid(real[:8], normalize=True)\n",
        "        img_grid_fake = torchvision.utils.make_grid(fake[:8], normalize=True)\n",
        "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
        "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xwe_cHeGIz1v"
      },
      "source": [
        "### StyleGAN defining parts "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:21:46.33918Z",
          "iopub.status.busy": "2022-03-13T12:21:46.33864Z",
          "iopub.status.idle": "2022-03-13T12:21:46.373846Z",
          "shell.execute_reply": "2022-03-13T12:21:46.373133Z",
          "shell.execute_reply.started": "2022-03-13T12:21:46.339141Z"
        },
        "id": "w2ZuWOCSIz1v",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
        "    '''\n",
        "    Function for calculating the gradient penalty: Given a tensor of images(real and fakes), \n",
        "    calculate their mixed critic score and penalize the gradient if its bigger than 1'''\n",
        "\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
        "    interpolated_images.requires_grad_(True)\n",
        "\n",
        "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "class WSLinear(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_features, out_features, gain=np.sqrt(2),\n",
        "    ):\n",
        "        super(WSLinear, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.scale = (gain / in_features)**0.5\n",
        "        self.bias = self.linear.bias\n",
        "        self.linear.bias = None\n",
        "\n",
        "        # initialize linear layer\n",
        "        nn.init.normal_(self.linear.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x * self.scale) + self.bias\n",
        "class MappingLayers(nn.Module):\n",
        "    '''\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        hidden_dim: the inner dimension, a scalar\n",
        "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
        "    '''\n",
        "    def __init__(self, z_dim, hidden_dim, w_dim):\n",
        "        super().__init__()\n",
        "        self.mapping = nn.Sequential(\n",
        "            PixelNorm(),\n",
        "           WSLinear(z_dim,hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            WSLinear(hidden_dim,hidden_dim),\n",
        "            nn.ReLU(),\n",
        "           WSLinear(hidden_dim,w_dim)\n",
        "\n",
        "        )\n",
        "    def forward(self, noise):\n",
        "        return self.mapping(noise)\n",
        "\n",
        "\n",
        "\n",
        "class NoiseInjection(nn.Module):\n",
        "    '''\n",
        "        channels: the number of channels the image has\n",
        "    '''\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(1,channels,1,1)\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        ''' Given an image, \n",
        "        returns the image with random noise added.\n",
        "        Parameters:\n",
        "            image:  shape (n_samples, channels, width, height)\n",
        "        '''\n",
        "        noise_shape = (image.shape[0],1,image.shape[2],image.shape[3])  \n",
        "        noise = torch.randn(noise_shape, device=image.device) # Creates the random noise\n",
        "        return image + self.weight * noise\n",
        "\n",
        "\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    '''\n",
        "    Param:\n",
        "        channels: the number of channels the image has\n",
        "        w_dim: the dimension of the intermediate noise vector\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, w_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Normalize the input per-dimension\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
        "\n",
        "        #defining the scale and bias weights for the styles\n",
        "        self.style_scale_transform = nn.Linear(w_dim, channels)\n",
        "        self.style_shift_transform = nn.Linear(w_dim, channels)\n",
        "\n",
        "    def forward(self, image, w):\n",
        "        '''\n",
        "        returns the normalized image that has been scaled and shifted by the style.\n",
        "        '''\n",
        "        normalized_image = self.instance_norm(image)\n",
        "        style_scale = self.style_scale_transform(w)[:, :, None, None] # fro broadcasting purpose\n",
        "        style_shift = self.style_shift_transform(w)[:, :, None, None] # fro broadcasting purpose\n",
        "        transformed_image = normalized_image*style_scale + style_shift\n",
        "        return transformed_image\n",
        "class MiniStyleGANGeneratorBlock(nn.Module):\n",
        "    '''\n",
        "    Values:\n",
        "        in_chan: the number of channels in the input\n",
        "        out_chan: the number of channels wanted in the output\n",
        "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
        "        kernel_size: the size of the convolving kernel\n",
        "        starting_size: the size of the starting image\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_chan, out_chan, w_dim, kernel_size, starting_size, padding=1,use_upsample=True):\n",
        "        super().__init__()\n",
        "        self.use_upsample = use_upsample\n",
        "        if self.use_upsample:\n",
        "            self.upsample = nn.Upsample((starting_size), mode='bilinear')\n",
        "        #self.conv = nn.Conv2d(in_chan, out_chan, kernel_size, padding='same') # Padding is used to maintain the image size\n",
        "        #self.conv=ConvBlock(in_chan, out_chan)\n",
        "        self.conv1 =WSConv2d(in_chan, out_chan,padding=padding,kernel_size=kernel_size)\n",
        "        self.conv2 = WSConv2d(out_chan, out_chan,kernel_size=kernel_size)\n",
        "        self.inject_noise1 = NoiseInjection(out_chan)\n",
        "        self.inject_noise2 = NoiseInjection(out_chan)\n",
        "        self.adain1 = AdaIN(out_chan, w_dim)\n",
        "        self.adain2 = AdaIN(out_chan, w_dim)\n",
        "        self.activation =  nn.LeakyReLU(0.2)\n",
        "      \n",
        "\n",
        "    def forward(self, x, w):\n",
        "        '''\n",
        "        Parameters:\n",
        "            x: the input into the generator, feature map of shape (n_samples, channels, width, height)\n",
        "            w: the intermediate noise vector\n",
        "        '''\n",
        "        if self.use_upsample:\n",
        "            x = self.upsample(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.inject_noise1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.adain1(x, w)\n",
        "        x = self.conv2(x)\n",
        "        x = self.inject_noise2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.adain2(x, w)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class WSConv2d(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels, out_channels, kernel_size=4, stride=1, padding=1, gain=np.sqrt(2)\n",
        "    ):\n",
        "        super(WSConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
        "        self.bias = self.conv.bias\n",
        "        self.conv.bias = None\n",
        "\n",
        "        # initialize conv layer\n",
        "        nn.init.normal_(self.conv.weight)\n",
        "        nn.init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)\n",
        "\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PixelNorm, self).__init__()\n",
        "        self.epsilon = 1e-8\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon)\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels,padding=1):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv1 = WSConv2d(in_channels, out_channels,kernel_size=4,padding=padding)\n",
        "        self.conv2 = WSConv2d(out_channels, out_channels,kernel_size=4)\n",
        "        self.leaky = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.leaky(self.conv1(x))\n",
        "        x = self.leaky(self.conv2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcQSg0BsUU0p"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:21:46.378301Z",
          "iopub.status.busy": "2022-03-13T12:21:46.377667Z",
          "iopub.status.idle": "2022-03-13T12:21:46.407055Z",
          "shell.execute_reply": "2022-03-13T12:21:46.406362Z",
          "shell.execute_reply.started": "2022-03-13T12:21:46.378263Z"
        },
        "id": "b9vAhamZUWaP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "factors=[1,1,1/2, 1/4,1/8]\n",
        "starting_sizes=[  4,8, 16,32]\n",
        "factors_gen=[1,1,1/2, 1/4,1/8]\n",
        "starting_sizes_gen=[ 4,8, 16,32]\n",
        "class StyleGANGenerator(nn.Module):\n",
        "    '''\n",
        "    mini StyleGAN Generator Class\n",
        "    Values:\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        map_hidden_dim: the mapping inner dimension, a scalar\n",
        "        w_dim: the dimension of the intermediate noise vector, a scalar\n",
        "        in_chan: the dimension of the constant input, usually w_dim\n",
        "        out_chan: the number of channels wanted in the output\n",
        "        kernel_size: the size of the convolving kernel\n",
        "        hidden_chan: the inner dimension, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, \n",
        "                 z_dim, \n",
        "\n",
        "                 map_hidden_dim,\n",
        "                 w_dim,\n",
        "                 in_chan,\n",
        "                 out_chan, \n",
        "                 kernel_size, \n",
        "                 hidden_chan,\n",
        "                 factors_gen,\n",
        "                 img_channels=3):\n",
        "        super().__init__()\n",
        "        self.factors_gen=factors_gen\n",
        "\n",
        "        self.block0 = MiniStyleGANGeneratorBlock(in_chan, hidden_chan, w_dim, kernel_size, starting_size=4, padding=2, use_upsample=False)\n",
        "        self.initial_rgb = WSConv2d(\n",
        "            in_chan, img_channels, kernel_size=1, stride=1, padding=0)\n",
        "        \n",
        "        self.map = MappingLayers(z_dim, map_hidden_dim, w_dim)\n",
        "        self.starting_constant = nn.Parameter(torch.randn(1, in_chan, 4, 4))# initial input\n",
        "\n",
        "        self.prog_blocks, self.rgb_layers = (\n",
        "            nn.ModuleList([]),\n",
        "            nn.ModuleList([self.initial_rgb]),\n",
        "        )\n",
        "\n",
        "        for i in range(1,len(self.factors_gen)-1):\n",
        "            in_c=int(in_chan*self.factors_gen[i])\n",
        "            out_c=int(in_chan*self.factors_gen[i+1])\n",
        "            self.prog_blocks.append( MiniStyleGANGeneratorBlock(in_c, out_c, w_dim, kernel_size, starting_sizes_gen[i],padding=2))\n",
        "            self.rgb_layers.append(\n",
        "                    WSConv2d(out_c, img_channels, kernel_size=1, stride=1, padding=0)\n",
        "                )\n",
        "\n",
        "        self.alpha = 0.2\n",
        "\n",
        "    def fade_in(self, alpha, upscaled, generated):\n",
        "        return torch.tanh(alpha * generated + (1 - alpha) * upscaled)\n",
        "    def forward(self, noise,alpha,steps,size):# steps determine the size of the image to return\n",
        "        x = self.starting_constant\n",
        "        w = self.map(noise)\n",
        "        out = self.block0(x,w)\n",
        "\n",
        "        if steps == 1:\n",
        "            return self.initial_rgb(out)\n",
        "        for step in range(steps-1):\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            ################################################      change if  ####################################\n",
        "            if(step!=2) or steps==4:# the step in wich othe block size didnt grow\n",
        "                upscaled = F.interpolate(out, scale_factor=2, mode='bilinear')\n",
        "            else:\n",
        "                upscaled=out\n",
        "            out = self.prog_blocks[step](upscaled,w)\n",
        "        final_upscaled = self.rgb_layers[steps-2 ](upscaled)\n",
        "        final_out = self.rgb_layers[steps-1](out)\n",
        "        return self.fade_in( alpha, final_upscaled, final_out)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, z_dim, in_channels, img_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
        "        self.leaky = nn.LeakyReLU(0.2)\n",
        "        for i in range(len(factors) - 1, 0, -1):\n",
        "            conv_in = int(in_channels * factors[i])\n",
        "            conv_out = int(in_channels * factors[i - 1])\n",
        "            self.prog_blocks.append(ConvBlock(conv_in, conv_out,padding=2))\n",
        "            self.rgb_layers.append(\n",
        "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
        "            )\n",
        "\n",
        "\n",
        "        self.initial_rgb = WSConv2d(\n",
        "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
        "        )\n",
        "        self.rgb_layers.append(self.initial_rgb)\n",
        "\n",
        "        self.avg_pool = nn.AvgPool2d(\n",
        "            kernel_size=2, stride=2\n",
        "        )\n",
        "\n",
        "        self.final_block = nn.Sequential(\n",
        "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),#\n",
        "            nn.LeakyReLU(0.2),\n",
        "            WSConv2d(\n",
        "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
        "            ),  \n",
        "        )\n",
        "\n",
        "    def fade_in(self, alpha, downscaled, out):\n",
        "\n",
        "        return alpha * out + (1 - alpha) * downscaled\n",
        "        \n",
        "\n",
        "    def minibatch_std(self, x):\n",
        "        '''\n",
        "        Compute Batch statistics and concatenate them with the the output,\n",
        "        Forces the Generaotor to be more diverse.\n",
        "        '''\n",
        "        batch_statistics = (\n",
        "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
        "        )\n",
        "\n",
        "        return torch.cat([x, batch_statistics], dim=1)\n",
        "\n",
        "    def forward(self, x, alpha, steps):\n",
        "\n",
        "        cur_step = len(self.prog_blocks) - steps+2 #our step starts from 1\n",
        "        out = self.leaky(self.rgb_layers[cur_step-1](x))\n",
        "        if steps == 1:\n",
        "            out = self.minibatch_std(out)\n",
        "            return self.final_block(out).view(out.shape[0], -1)\n",
        "\n",
        "        downscaled = self.leaky(self.rgb_layers[cur_step](self.avg_pool(x)))\n",
        "\n",
        "        out = self.avg_pool(self.prog_blocks[cur_step-1](out))\n",
        "\n",
        "        out = self.fade_in(alpha, downscaled, out)\n",
        "\n",
        "        for step in range(cur_step, len(self.prog_blocks)):\n",
        "          \n",
        "            out = self.prog_blocks[step](out)\n",
        "\n",
        "            if cur_step !=1 :\n",
        "                out = self.avg_pool(out)\n",
        "        out = self.minibatch_std(out)\n",
        "        return self.final_block(out).view(out.shape[0], -1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrK61jDbUmwg"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:21:46.408619Z",
          "iopub.status.busy": "2022-03-13T12:21:46.408333Z",
          "iopub.status.idle": "2022-03-13T12:21:46.463675Z",
          "shell.execute_reply": "2022-03-13T12:21:46.462832Z",
          "shell.execute_reply.started": "2022-03-13T12:21:46.408581Z"
        },
        "id": "4KhquhTXtpWL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os  \n",
        "import numpy as np\n",
        "map_hidden_dim=256\n",
        "w_dim=100\n",
        "in_chan=128\n",
        "#kernel_size=3\n",
        "hidden_chan=128\n",
        "kernel_size=4\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "batch_size = 32\n",
        "BATCH_SIZES = [32,32,16, 8,4] \n",
        "prog_epochs = [0]+[30] + [30]+[40] + [40]\n",
        "#prog_epochs=[1]*len(BATCH_SIZES)        ### for quick testing \n",
        "z_dim = 128\n",
        "out_chan = 3\n",
        "truncation = 0.7\n",
        "lamda_gp=10\n",
        "lr=1e-3\n",
        "beta_1, beta_2=0.0,0.99\n",
        "sizes=[ 0, 4,8,16, 32]\n",
        "from math import log2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe094nNwelUF"
      },
      "source": [
        "#### Tensorboard functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:21:46.467334Z",
          "iopub.status.busy": "2022-03-13T12:21:46.467105Z",
          "iopub.status.idle": "2022-03-13T12:21:46.47698Z",
          "shell.execute_reply": "2022-03-13T12:21:46.476286Z",
          "shell.execute_reply.started": "2022-03-13T12:21:46.467308Z"
        },
        "id": "hJxOzI6GtpWL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def plot_to_tensorboard(\n",
        "    writer, loss_critic, loss_gen, real, fake, tensorboard_step\n",
        "):\n",
        "    writer.add_scalar(\"Loss Critic\", loss_critic, global_step=tensorboard_step)\n",
        "    writer.add_scalar(\"Loss gen\", loss_gen, global_step=tensorboard_step)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # take out (up to) 8 examples to plot\n",
        "        img_grid_real = torchvision.utils.make_grid(real[:12], normalize=True)\n",
        "        img_grid_fake = torchvision.utils.make_grid(fake[:12], normalize=True)\n",
        "        writer.add_image(\"Real\", img_grid_real, global_step=tensorboard_step)\n",
        "        writer.add_image(\"Fake\", img_grid_fake, global_step=tensorboard_step)\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=\"cuda\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # save the lr of the current step,\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "CHECKPOINT_GEN = \"generator.pth\"\n",
        "CHECKPOINT_CRITIC = \"critic_.pth\"\n",
        "SAVE_MODEL = True\n",
        "LOAD_MODEL = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-13T12:21:46.478592Z",
          "iopub.status.busy": "2022-03-13T12:21:46.478169Z",
          "iopub.status.idle": "2022-03-13T12:22:40.270488Z",
          "shell.execute_reply": "2022-03-13T12:22:40.269321Z",
          "shell.execute_reply.started": "2022-03-13T12:21:46.478503Z"
        },
        "id": "opI5BfB4UmI-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "fixed_noise = get_truncated_noise(35, z_dim,truncation).to(device)\n",
        "def get_loader(image_size):\n",
        "    out_chan=3 #image channels \n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.Normalize(\n",
        "                [0.5 for _ in range(out_chan)],\n",
        "                [0.5 for _ in range(out_chan)],\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    batch_size =128\n",
        "    dataset =  torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "    small = list(range(300))                                     ### for quick testing \n",
        "    dataset = torch.utils.data.Subset(dataset, small)            ### for quick testing \n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n",
        "\n",
        "\n",
        "\n",
        "def train(num_epochs, critic,gen, loader, dataset, step, alpha, opt_critic, opt_gen, tensorboard_step, writer, scaler_gen, scaler_critic):\n",
        "    loop = tqdm(loader, leave=True)\n",
        "    crit_rep=3\n",
        "    display_step=500\n",
        "    for batch_idx, (real, _) in enumerate(loop) :\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        m=0\n",
        "        for _ in range(crit_rep):\n",
        "            with torch.cuda.amp.autocast():# add crit-repeats if neccessary \n",
        "                opt_critic.zero_grad()\n",
        "                noise = get_truncated_noise(cur_batch_size, z_dim,truncation).to(device)\n",
        "                fake = gen(noise, alpha, step,real.shape[-1])\n",
        "                critic_real = critic(real, alpha, step)\n",
        "                critic_fake = critic(fake.detach(), alpha, step)\n",
        "\n",
        "\n",
        "                gp = gradient_penalty(critic, real, fake, alpha, step, device=device)\n",
        "                loss_critic =  (-(torch.mean(critic_real) - torch.mean(critic_fake)) + lamda_gp * gp+ (0.001 * torch.mean(critic_real ** 2)) )\n",
        "                scaler_critic.scale(loss_critic).backward(retain_graph=True)#\n",
        "                scaler_critic.step(opt_critic)\n",
        "                scaler_critic.update()\n",
        "        with torch.cuda.amp.autocast():# for mixed precision\n",
        "            gen_fake = critic(fake, alpha, step)\n",
        "            loss_gen = -torch.mean(gen_fake)\n",
        "          \n",
        "        opt_gen.zero_grad()\n",
        "        scaler_gen.scale(loss_gen).backward()\n",
        "        scaler_gen.step(opt_gen)\n",
        "        scaler_gen.update()\n",
        "\n",
        "        # Update alpha and ensure less than 1\n",
        "        alpha += cur_batch_size / (\n",
        "            (prog_epochs[step] * 0.5) * len(dataset)\n",
        "        )\n",
        "        alpha = min(alpha, 1)\n",
        "\n",
        "        if batch_idx % 500 == 0:\n",
        "            with torch.no_grad():\n",
        "                print(\"fixed_noise \",fixed_noise.shape)\n",
        "                fixed_fakes = gen(fixed_noise, alpha, step,real.shape[-1]) * 0.5 + 0.5\n",
        "                show_tensor_images(fixed_fakes,num_epochs, num_images=fixed_fakes.shape[0], size=fixed_fakes.shape[1:])\n",
        "            plot_to_tensorboard(\n",
        "                writer,\n",
        "                loss_critic.item(),\n",
        "                loss_gen.item(),\n",
        "                real.detach(),\n",
        "                fixed_fakes.detach(),\n",
        "                tensorboard_step,\n",
        "            )\n",
        "            tensorboard_step += 1\n",
        "            loop.set_postfix(\n",
        "            gp=gp.item(),\n",
        "            loss_critic=loss_critic.item(),\n",
        "        )\n",
        " \n",
        "        ############  for printing learning curves while training   ##############\n",
        "\n",
        "            #crit_mean = sum(c_loss[-display_step:]) / display_step\n",
        "            #step_bins = 20\n",
        "           # num_examples = (len(gen_mean_loss) // step_bins) * step_bins\n",
        "           # plt.plot(\n",
        "           #     range(num_examples // step_bins), \n",
        "           #     torch.Tensor(gen_mean_loss[:num_examples]).view(-1, step_bins).mean(1),\n",
        "           #     label=\"Generator Loss\"\n",
        "          #  )\n",
        "          #  plt.plot(\n",
        "          #      range(num_examples // step_bins), \n",
        "          #      torch.Tensor(crit_mean_loss[:num_examples]).view(-1, step_bins).mean(1),\n",
        "          #      label=\"Critic Loss\"\n",
        "          #  )\n",
        "          #  plt.legend()\n",
        "          #  plt.show()\n",
        "\n",
        "        loop.set_postfix(\n",
        "            gp=gp.item(),\n",
        "            loss_critic=loss_critic.item(),\n",
        "        )\n",
        "                   \n",
        "    return tensorboard_step, alpha\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "def main(z_dim, map_hidden_dim,w_dim,in_chan , out_chan,kernel_size,device,lr,prog_epochs,factors_gen):  \n",
        "    gen = StyleGANGenerator(z_dim, map_hidden_dim,w_dim,in_chan , out_chan,kernel_size,hidden_chan,factors_gen).to(device)\n",
        "    critic = Discriminator(z_dim, in_chan).to(device)\n",
        "\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
        "    opt_critic = optim.Adam(\n",
        "        critic.parameters(), lr=lr, betas=(beta_1, beta_2)\n",
        "    )\n",
        "    scaler_critic = torch.cuda.amp.GradScaler()\n",
        "    scaler_gen = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # for tensorboard plotting\n",
        "    writer = SummaryWriter(f\"logs\")\n",
        "    gen.train()\n",
        "    critic.train()\n",
        "    writer = SummaryWriter(f\"logs/gan1\")\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_GEN, gen, opt_gen, lr,\n",
        "        )\n",
        "        load_checkpoint(\n",
        "            CHECKPOINT_CRITIC, critic, opt_critic, lr,\n",
        "        )\n",
        "    START_TRAIN_AT_IMG_SIZE = 32 #for cifar10\n",
        "    tensorboard_step = 0\n",
        "    # start at step that corresponds to img size that we set in config\n",
        "    step = 1\n",
        "    print( gen)\n",
        "    print(\"####################################################################################################################################\")\n",
        "    print(critic)\n",
        "    for num_epochs in prog_epochs[step:]:\n",
        "        print(\"step\", step)\n",
        "        alpha = 1e-5  # start with very low alpha\n",
        "        loader, dataset = get_loader(sizes[step])  # 4->1 8->2, 16->3, 32->4\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "            tensorboard_step, alpha = train(num_epochs,\n",
        "                critic,\n",
        "                gen,\n",
        "                loader,\n",
        "                dataset,\n",
        "                step,\n",
        "                alpha,\n",
        "                opt_critic,\n",
        "                opt_gen,\n",
        "                tensorboard_step,\n",
        "                writer,\n",
        "                scaler_gen,scaler_critic\n",
        "            )\n",
        "            if SAVE_MODEL:\n",
        "                save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN)\n",
        "                save_checkpoint(critic, opt_critic, filename=CHECKPOINT_CRITIC)\n",
        "        step += 1  # progress to the next img size\n",
        "\n",
        "\n",
        "    with open(Pkl_gen, 'wb') as file:  \n",
        "        pickle.dump(gen, file)\n",
        "    with open(Pkl_critic, 'wb') as file:  \n",
        "        pickle.dump(critic, file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "main(z_dim, map_hidden_dim,w_dim,in_chan , out_chan,kernel_size,device,lr,prog_epochs,factors_gen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9Bt0umEfQM9"
      },
      "source": [
        "##Testing the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T12:22:40.271718Z",
          "iopub.status.idle": "2022-03-13T12:22:40.272438Z",
          "shell.execute_reply": "2022-03-13T12:22:40.272198Z",
          "shell.execute_reply.started": "2022-03-13T12:22:40.27217Z"
        },
        "id": "eC45eC2pIz11",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "### loading the models \n",
        "\n",
        "with open(Pkl_gen, 'rb') as file:  \n",
        "    gen = pickle.load(file)\n",
        "with open(Pkl_critic, 'rb') as file:  \n",
        "    critic = pickle.load(file)\n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-03-13T12:22:40.273556Z",
          "iopub.status.idle": "2022-03-13T12:22:40.274328Z",
          "shell.execute_reply": "2022-03-13T12:22:40.274105Z",
          "shell.execute_reply.started": "2022-03-13T12:22:40.274079Z"
        },
        "id": "P3q3qCCTIz13",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for i in range(200):\n",
        "\n",
        "    step=4 # so that the generated images are 32*32\n",
        "    testing_noise = get_truncated_noise(5, z_dim,truncation).to(device)\n",
        "    with torch.no_grad():\n",
        "        print(\"fixed_noise \",testing_noise.shape)\n",
        "        alpha=1 #so that theres no interpolation of 16*16 images\n",
        "        generated_fakes = gen(testing_noise, alpha, step,32) * 0.5 + 0.5\n",
        "        show_tensor_images(generated_fakes,20, num_images=generated_fakes.shape[0], size=generated_fakes.shape[1:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_W8kJA8LnIU"
      },
      "source": [
        "##FID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_py9fNJRNB1"
      },
      "outputs": [],
      "source": [
        "Pkl_gen = \"gen.pkl\" \n",
        "Pkl_critic = \"critic.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6Kj4dmgRM-k"
      },
      "outputs": [],
      "source": [
        "#Loading our pretrained model\n",
        "\n",
        "with open(Pkl_gen, 'rb') as file:  \n",
        "    gen = pickle.load(file)\n",
        "with open(Pkl_critic, 'rb') as file:  \n",
        "    critic = pickle.load(file)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gzq6GQOCLoHV"
      },
      "outputs": [],
      "source": [
        "def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n",
        "                    cuda=False):\n",
        "    model.eval()\n",
        "    act=np.empty((len(images), dims))\n",
        "    \n",
        "    if cuda:\n",
        "        batch=images.cuda()\n",
        "    else:\n",
        "        batch=images\n",
        "    pred = model(batch)[0]\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)  \n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uakpEWgJLpWd"
      },
      "outputs": [],
      "source": [
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    \n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    \n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tldgONmkLpTq"
      },
      "outputs": [],
      "source": [
        "def calculate_fretchet(images_real,images_fake,model):\n",
        "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
        "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
        "    \n",
        "     \"\"\"get fretched distance\"\"\"\n",
        "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
        "     return fid_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKHIBdonPUe0"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.utils as vutils\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "from scipy import linalg\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NFSFaOCPL2l"
      },
      "outputs": [],
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False):\n",
        "        \n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        \n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp\n",
        "    \n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "model = InceptionV3([block_idx])\n",
        "model=model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV8YH5CjLpQ8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_loader_FID(image_size,batch_size):\n",
        "    out_chan=3 #image channels \n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.Normalize(\n",
        "                [0.5 for _ in range(out_chan)],\n",
        "                [0.5 for _ in range(out_chan)],\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    dataset =  torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n",
        "gen.eval()\n",
        "n_samples = 50000 # The total number of samples\n",
        "batch_size = 256 # Samples per iteration\n",
        "\n",
        "dataloader, dataset = get_loader_FID(32,batch_size) \n",
        "cur_samples = 0\n",
        "fid=[]\n",
        "with torch.no_grad(): # The loop is mainly to observe execution time, plus getting the mean FID Value\n",
        "    try:\n",
        "        \n",
        "        for real_example, _ in tqdm(dataloader, total=n_samples // batch_size): \n",
        "            real_samples = real_example.to('cpu')\n",
        "            testing_noise = get_truncated_noise(len(real_example), z_dim,truncation).to(device)\n",
        "            alpha=1 #so that theres no interpolation of 16*16 images\n",
        "            fake_samples = gen(testing_noise, alpha, step,32) * 0.5 + 0.5\n",
        "            fake_samples =     fake_samples.to('cpu')\n",
        "            cur_samples += len(real_samples)\n",
        "            fretchet_dist=calculate_fretchet(real_samples,fake_samples,model) \n",
        "            fid.append(fretchet_dist)\n",
        "            print(\"this step\",fretchet_dist)\n",
        "            print(\"total\",np.mean(fid))\n",
        "            if cur_samples >= n_samples:\n",
        "                break\n",
        "    except:\n",
        "        print(\"Error in loop\")\n",
        "\n",
        "print(\"FID\",np.mean(fid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS3QNrUDLpOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPGrGHSJLpLt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKmN-JkrLpI8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Mini_StyleGAN (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
